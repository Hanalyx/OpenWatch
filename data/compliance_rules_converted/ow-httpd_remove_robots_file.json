{
  "_id": "ow-httpd_remove_robots_file",
  "rule_id": "ow-httpd_remove_robots_file",
  "scap_rule_id": "xccdf_org.ssgproject.content_rule_httpd_remove_robots_file",
  "parent_rule_id": null,
  "metadata": {
    "name": "The robots.txt Files Must Not Exist",
    "description": "Remove any <tt>robots.txt</tt> files that may exist with any web content.\nOther methods must be employed if there is information on the web site that\nneeds protection from search engines and public view. Inspect all instances of\n<tt>DocumentRoot</tt> and <tt>Alias</tt> and remove any <tt>robots.txt</tt> file.\n<pre>$ sudo rm -f path/to/robots.txt</pre>",
    "rationale": "Search engines are constantly at work on the Internet. Search engines are\naugmented by agents, often referred to as spiders or bots, which endeavor to\ncapture and catalog web-site content. In turn, these search engines make the\ncontent they obtain and catalog available to any public web user.\n<br /><br />\nTo request\nthat a well behaved search engine not crawl and catalog a site, the web site may\ncontain a file called robots.txt. This file contains directories and files that\nthe web server SA desires not be crawled or cataloged, but this file can also be\nused, by an attacker or poorly coded search engine, as a directory and file\nindex to a site. This information may be used to reduce an attacker's time\nsearching and traversing the web site to find files that might be relevant. If\ninformation on the web site needs to be protected from search engines and public\nview, other methods must be used.",
    "source": {
      "upstream_id": "httpd_remove_robots_file",
      "complianceascode_version": "0.1.73",
      "source_file": "converted_from_yaml",
      "cce_id": "",
      "imported_at": "2025-10-03T10:45:55.845205+00:00"
    }
  },
  "abstract": false,
  "severity": "medium",
  "category": "system_hardening",
  "security_function": "access_control",
  "tags": [
    "severity_medium",
    "scap",
    "converted",
    "ssg"
  ],
  "frameworks": {},
  "platform_implementations": {},
  "platform_requirements": {
    "required_capabilities": [],
    "excluded_environments": []
  },
  "check_type": "scap",
  "check_content": {
    "scap_rule_id": "xccdf_org.ssgproject.content_rule_httpd_remove_robots_file",
    "method": "xccdf_evaluation",
    "expected_result": "pass"
  },
  "fix_available": false,
  "fix_content": {},
  "manual_remediation": "Remove any <tt>robots.txt</tt> files that may exist with any web content.\nOther methods must be employed if there is information on the web site that\nneeds protection from search engines and public view. Inspect all instances of\n<tt>DocumentRoot</tt> and <tt>Alias</tt> and remove any <tt>robots.txt</tt> file.\n<pre>$ sudo rm -f path/to/robots.txt</pre>",
  "remediation_complexity": "medium",
  "remediation_risk": "low",
  "dependencies": {
    "requires": [],
    "conflicts": [],
    "related": []
  },
  "source_file": "linux_os/guide/services/http/securing_httpd/httpd_secure_content/httpd_remove_robots_file/rule.yml",
  "source_hash": "sha256:7de00621cbf8e8d1",
  "version": "2024.2",
  "imported_at": "2025-10-03T10:45:55.845303+00:00",
  "updated_at": "2025-10-03T10:45:55.845305+00:00",
  "identifiers": {}
}